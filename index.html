<!DOCTYPE html>
<html lang="zh-CN">

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="Home page of REMEX">
    <meta name="author" content="WeiQM">
    <link rel="icon" href="images/logo/RMX_16.ico">
    <title>REMEX - Remote sensing + Medical imaging + X-features</title>
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="style/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="style/jquery.bxslider.css" rel="stylesheet">
    <link href="style/style.css" rel="stylesheet">
  </head>

  <body>
    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">  
          <ul class="nav navbar-nav">
            <li class="active"><a href="index.html">Home</a></li>
            <li><a href="people.html">People</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="downloads.html">Downloads</a></li>
            <li><a href="contact.html">Contact Us</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li class="active"><a href="index.html">English</a></li>
            <li><a href="html/cn/index.html">中文</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="index.html"><img src="images/logo/logo_w.png" alt="Logo" width="80px"/></a></li>
          </ul>
        </div>
      </div>
    </nav>
    <div class="container">
    <header>
      <!--
      <a href="index.html"><img src="images/logo.png"  width="256px"></a>
      -->
    </header>
    <!--
    <section class="main-slider">
      <ul class="bxslider">
        <li><div class="slider-item"><img src="images/logo/logo_c.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_m.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_y.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_k.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_r.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_g.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_b.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
      </ul>
    </section>
    -->
    <section>
      <div class="row">
        <!-- Main Page -->
        <div class="col-md-8">
          <introduce class="content-block">
            <div class="block-body">
              <img src="images/logo.png" alt="Logo" width="512px">
              <p><br>
                <b>REMEX</b> (<b>Re</b>mote sensing and <b>Me</b>dical imaging with <b>X</b>-features) is a research group directed by Prof. Zhiguo Jiang. The main research interest includes image processing, computer vision, pattern recognition,  deep learning, and their applications on remote sensing, medical imaging.
              </p>
              
              <div class="block-image">
                <img src="images/photo/Team1.jpg" alt="Team photo">
              </div>
              <hr/>
              <h3 align="middle"> <font color="FF0000">【New】</font><a href="html\cn\index.html">REMEX Lab summer Camp 暑期夏令营 2022 </a></h3>
              <hr/>
              <h3 align="left">Recently Published</h3><br />
              <div class="block-text">
            <table><tbody>


            <tr> <!-- An Paper -->
                <p>
                    <b>Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification</b> <a href="https://zhengyushan.github.io" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        <a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng*</a>, Jun Shi, Fengying Xie, Zhiguo Jiang
                    </i></font>
                    <br>
                    MICCAI 2022, Accepted.
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="https://zhengyushan.github.io" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhengMICCAI2022Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhengMICCAI2022Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="https://zhengyushan.github.io" target="_blank">Code</a>
                </p>
                <p id="ZhengMICCAI2022Abs" class="abstract" style="display: none;">

                </p>
                <pre xml:space="preserve" id="ZhengMICCAI2022Bib" class="bibtex" style="display: none;">
  @Article{zheng2022encoding,
    author  = {Yushan Zheng, Jun Shi, Fengying Xie, Zhiguo Jiang},
    title   = {Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification},
    accepted = {MICCAI 2022}
  }
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('ZhengMICCAI2022Abs');
                  hideblock('ZhengMICCAI2022Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->

            <tr> <!-- An Paper -->
                <p>
                    <b>Lesion-Aware Contrastive Representation Learning For Histopathology Whole Slide Images Analysis</b> <a href="" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        Jun Li, <a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng*</a>, Kun Wu, Jun Shi, Fengying Xie, Zhiguo Jiang
                    </i></font>
                    <br>
                    MICCAI 2022, Accepted.
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="https://zhengyushan.github.io" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('LiMICCAI2022Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('LiMICCAI2022Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="" target="_blank">Code</a>
                </p>
                <p id="LiMICCAI2022Abs" class="abstract" style="display: none;">

                </p>
                <pre xml:space="preserve" id="LiMICCAI2022Bib" class="bibtex" style="display: none;">
  @Article{li2022encoding,
    author  = {Jun Li, Yushan Zheng*, Kun Wu, Jun Shi, Fengying Xie, Zhiguo Jiang},
    title   = {Lesion-Aware Contrastive Representation Learning For Histopathology Whole Slide Images Analysis},
    accepted = {MICCAI 2022}
  }
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('LiMICCAI2022Abs');
                  hideblock('LiMICCAI2022Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->

            <tr> <!-- An Paper -->
                <p>
                    <b>Multi-Frame Super-Resolution With Raw Images Via Modified Deformable Convolution</b> <a href="" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        Gongzhe Li, Linwei Qiu,<a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>, Fengying Xie, Zhiguo Jiang
                    </i></font>
                    <br>
                    IEEE ICASSP 2022
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhangICASSP2022Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhangICASSP2022Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="" target="_blank">Code</a>
                </p>
                <p id="ZhangICASSP2022Abs" class="abstract" style="display: none;">

                </p>
                <pre xml:space="preserve" id="ZhangICASSP2022Bib" class="bibtex" style="display: none;">
@Article{
}
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('ZhangICASSP2022Abs');
                  hideblock('ZhangICASSP2022Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->

            <tr> <!-- An Paper -->
                <p>
                    <b>A Two-Stage Shake-Shake Network for Long-tailed Recognition of SAR Aerial View Objects</b> <a href="" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>
                    </i></font>
                    <br>
                    18th IEEE Workshop on Perception Beyond the Visible Spectrum (PBVS Workshop 2022)in conjunction with CVPR 2022
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhangPBVS2022Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhangPBVS2022Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="" target="_blank">Code</a>
                </p>
                <p id="ZhangPBVS2022Abs" class="abstract" style="display: none;">

                </p>
                <pre xml:space="preserve" id="ZhangPBVS2022Bib" class="bibtex" style="display: none;">
@Article{
}
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('ZhangPBVS2022Abs');
                  hideblock('ZhangPBVS2022Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->



            <tr> <!-- An Paper -->
                <p>
                    <b>Few-Shot Multi-Class Ship Detection in Remote Sensing Images Using Attention Feature Map and Multi-Relation Detector</b> <a href="https://www.mdpi.com/2072-4292/14/12/2790" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a> , Xingyu Zhang, Gang Meng, Chen Guo, Zhiguo Jiang
                    </i></font>
                    <br>
                    Remote Sens. 2022
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="https://doi.org/10.3390/rs14122790" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhangRS2022Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhangRS2022Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="" target="_blank">Code</a>
                </p>
                <p id="ZhangRS2022Abs" class="abstract" style="display: none;">
                    Monitoring and identification of ships in remote sensing images is of great significance for port management, marine traffic, marine security, etc. However, due to small size and complex background, ship detection in remote sensing images is still a challenging task. Currently, deep-learning-based detection models need a lot of data and manual annotation, while training data containing ships in remote sensing images may be in limited quantities. To solve this problem, in this paper, we propose a few-shot multi-class ship detection algorithm with attention feature map and multi-relation detector (AFMR) for remote sensing images. We use the basic framework of You Only Look Once (YOLO), and use the attention feature map module to enhance the features of the target. In addition, the multi-relation head module is also used to optimize the detection head of YOLO. Extensive experiments on publicly available HRSC2016 dataset and self-constructed REMEX-FSSD dataset validate that our method achieves a good detection performance.
                </p>
                <pre xml:space="preserve" id="ZhangRS2022Bib" class="bibtex" style="display: none;">
@Article{rs14122790,
    AUTHOR = {Zhang, Haopeng and Zhang, Xingyu and Meng, Gang and Guo, Chen and Jiang, Zhiguo},
    TITLE = {Few-Shot Multi-Class Ship Detection in Remote Sensing Images Using Attention Feature Map and Multi-Relation Detector},
    JOURNAL = {Remote Sensing},
    VOLUME = {14},
    YEAR = {2022},
    NUMBER = {12},
    ARTICLE-NUMBER = {2790},
    URL = {https://www.mdpi.com/2072-4292/14/12/2790},
    ISSN = {2072-4292},
    DOI = {10.3390/rs14122790}
}
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('ZhangRS2022Abs');
                  hideblock('ZhangRS2022Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->
              <tr> <!-- An Paper -->
                <p>
                    <b>Encoding Histopathology Whole Slide Images with Location-aware Graphs for Diagnostically Relevant Regions Retrieval</b> <a href="https://doi.org/10.1016/j.media.2021.102308" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        <a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng</a>, Zhiguo Jiang*, Jun Shi, Fengying Xie, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>, Wei Luo, Dingyi Hu, Shujiao Sun, Zhongmin Jiang, and Chenghai Xue
                    </i></font>
                    <br>
                    Medical Image Analysis, 2022
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_zheng_mia_2021.pdf" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhengMIA2021Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhengMIA2021Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="https://github.com/Zhengyushan/lagenet" target="_blank">Code</a>
                </p>
                <p id="ZhengMIA2021Abs" class="abstract" style="display: none;">
                    Content-based histopathological image retrieval (CBHIR) has become popular in recent years in histopathological image analysis. CBHIR systems provide auxiliary diagnosis information for pathologists by searching for and returning regions that are contently similar to the region of interest (ROI) from a pre-established database. It is challenging and yet significant in clinical applications to retrieve diagnostically relevant regions from a database consisting of histopathological whole slide images (WSIs). In this paper, we propose a novel framework for regions retrieval from WSI database based on location-aware graphs and deep hash techniques. Compared to the present CBHIR framework, both structural information and global location information of ROIs in the WSI are preserved by graph convolution and self-attention operations, which makes the retrieval framework more sensitive to regions that are similar in tissue distribution. Moreover, benefited from the graph structure, the proposed framework has good scalability for both the size and shape variation of ROIs. It allows the pathologist to define query regions using free curves according to the appearance of tissue. Thirdly, the retrieval is achieved based on the hash technique, which ensures the framework is efficient and adequate for practical large-scale WSI database. The proposed method was evaluated on an in-house endometrium dataset with 2650 WSIs and the public ACDC-LungHP dataset. The experimental results have demonstrated that the proposed method achieved a mean average precision above 0.667 on the  endometrium dataset and above 0.869 on the ACDC-LungHP dataset in the task of irregular region retrieval, which are superior to the state-of-the-art methods. The average retrieval time from a database containing 1855 WSIs is 0.752 ms.
                </p>
                <pre xml:space="preserve" id="ZhengMIA2021Bib" class="bibtex" style="display: none;">
  @Article{zheng2022encoding,
    author  = {Zheng, Yushan and Jiang, Zhiguo and Shi, Jun and Xie, Fengying and Zhang, Haopeng and
                Luo, Wei and Hu, Dingyi and Sun, Shujiao and Jiang, Zhongmin and Xue, Chenghai},
    title   = {Encoding histopathology whole slide images with location-aware graphs for diagnostically relevant regions retrieval},
    journal = {Medical Image Analysis},
    year    = {2022},
    volumn  = {76},
    pages   = {102308},
    doi     = {https://doi.org/10.1016/j.media.2021.102308},
  }
                </pre>
                    <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('ZhengMIA2021Abs');
                  hideblock('ZhengMIA2021Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->   
            <tr> <!-- An Paper -->
              <p>
                  <b>Weakly Supervised Histopathological Image Representation Learning based on Contrastive Dynamic Clustering</b> <!--a href="https://doi.org/10.1016/j.media.2021.102308" target="_blank"><i class="fa fa-external-link"></i></a-->
                  <br>
                  <font size="3pt" face="Georgia"><i>
                      Jun Li, Zhiguo Jiang, <a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng*</a>, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>, Jun Shi, Dingyi Hu,  Wei Luo, Zhongmin Jiang, and Chenghai Xue
                  </i></font>
                  <br>
                  SPIE Medical Imaging, 2022
                  <br>
                  <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_li_spiemi_2022.pdf" target="_blank">PDF</a>
                  <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('LiSPIEMI2022Abs')">Abstract</a> &nbsp;
                  <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('LiSPIEMI2022Bib')">BibTeX</a> &nbsp;
                  <i class="fa fa-github"></i> <a href="https://github.com/junl21/cdc" target="_blank">Code</a>
              </p>
              <p id="LiSPIEMI2022Abs" class="abstract" style="display: none;">
                  Feature representations of histopathology whole slide images (WSIs) are crucial to the downstream applications
                  for computer-aided cancer diagnosis, including whole slide image classification, region of interest detection, hash
                  retrieval, prognosis analysis, and other high-level inference tasks. State-of-the-art methods for whole slide image
                  feature extraction generally rely on supervised learning algorithms based on fine-grained manual annotations,
                  unsupervised learning algorithms without annotation, or directly use pre-trained features. At present, there is
                  a lack of research on weakly supervised feature learning methods that only utilize WSI-level labeling. In this
                  paper, we propose a weakly supervised framework that learns the feature representations of various lesion areas
                  from histopathology whole slide images. The proposed framework consists of a contrastive learning network as
                  the backbone and a designed contrastive dynamic clustering (CDC) module to embedding the lesion information
                  into the feature representations. The proposed method was evaluated on a large scale endometrial whole slide
                  image dataset. The experimental results have demonstrated that our method can learn discriminative feature
                  representations for histopathology image classification and the quantitative performance of our method is close
                  to the fully-supervision learning methods
              </p>
              <pre xml:space="preserve" id="LiSPIEMI2022Bib" class="bibtex" style="display: none;">
@inproceedings{li2021weakly,
  author    = {Jun Li, Zhiguo Jiang, Yushan Zheng, Haopeng Zhang, Jun Shi, Dingyi Hu,
               Wei Luo, Zhongmin Jiang, and Chenghai Xue},
  title     = {Weakly Supervised Histopathological Image Representation Learning based on Contrastive Dynamic Clustering},
  booktitle = {SPEI Medical Imaging 2022},
  year      = {2022},
}
              </pre>
                  <script language="javascript" type="text/javascript" xml:space="preserve">
                hideblock('LiSPIEMI2022Abs');
                hideblock('LiSPIEMI2022Bib');
              </script>
              </td>
          </tr> <!-- Paper End Here -->

            <tr> <!-- An Paper -->
                <p>
                    <b>Few-Shot Ship Classification in Optical Remote Sensing Images Using Nearest Neighbor Prototype Representation</b> <a href="https://doi.org/10.1109/JSTARS.2021.3066539" target="_blank"><i class="fa fa-external-link"></i></a>
                    <br>
                    <font size="3pt" face="Georgia"><i>
                        Jiawei Shi, Zhiguo Jiang, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>
                    </i></font>
                    <br>
                    IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2021
                    <br>
                    <i class="fa fa-file-pdf-o"></i> <a href="https://doi.org/10.1109/JSTARS.2021.3066539" target="_blank">PDF</a>
                    <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ShiRS2021Abs')">Abstract</a> &nbsp;
                    <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ShiRS2021Bib')">BibTeX</a> &nbsp;
                    <i class="fa fa-github"></i> <a href="" target="_blank">Code</a>
                </p>
                <p id="ShiRS2021Abs" class="abstract" style="display: none;">
                    With the development of ship detection in optical remote sensing images, it is convenient to obtain accurate detection results and ship images. Owing to the superior performance of convolutional neural networks (CNNs), one way to acquire the category of ship is to train a classifier using numerous ship images. However, the classification performance of CNN may degrade in the case of a small number of training samples. To solve this problem, we propose a metric-based few-shot method to generate novel concept (class) representation using nearest neighbor prototype. Different from image-to-image measure in common few-shot methods, we use an image-to-feature measure. We map small number of samples to the feature space through CNN, and generate prototypes by computing nearest neighbor value on each dimension of the feature separately. Our method is validated on patch-level ship image dataset, a reproduced ship classification dataset based on HRSC2016. The experimental results demonstrate the accuracy and robustness of our method for ship classification with a small amount of labeled data.
                </p>
                <pre xml:space="preserve" id="ShiRS2021Bib" class="bibtex" style="display: none;">
@ARTICLE{9380722,
  author={Shi, Jiawei and Jiang, Zhiguo and Zhang, Haopeng},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  title={Few-Shot Ship Classification in Optical Remote Sensing Images Using Nearest Neighbor Prototype Representation},
  year={2021},
  volume={14},
  number={},
  pages={3581-3590},
  doi={10.1109/JSTARS.2021.3066539}}
                </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
                  hideblock('ShiRS2021Abs');
                  hideblock('ShiRS2021Bib');
                </script>
                </td>
            </tr> <!-- Paper End Here -->

          <tr> <!-- An Paper -->
            <p>
                <b>Hyperspectral Image Classification using Feature Fusion Hypergraph Convolution Neural Network</b> <a href="https://doi.org/10.1109/TGRS.2021.3123423" target="_blank"><i class="fa fa-external-link"></i></a>
                <br>
                <font size="3pt" face="Georgia"><i>
                    Zhongtian Ma, Zhiguo Jiang, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>
                </i></font>
                <br>
                IEEE Transactions on Geoscience and Remote Sensing, 2021
                <br>
                <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_ma_tgrs_2021.pdf" target="_blank">PDF</a>
                <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('MaTGRS2021Abs')">Abstract</a> &nbsp;
                <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('MaTGRS2021Bib')">BibTeX</a> &nbsp;
                <i class="fa fa-github"></i> <a href="https://github.com/mztmzt/F2HNN" target="_blank">Code</a>
            </p>
            <p id="MaTGRS2021Abs" class="abstract" style="display: none;">
                Convolutional neural networks (CNN) and graph representation learning are two common methods for hyperspectral image (HSI) classification. Recently, graph convolutional neural networks (GCN), a combination of CNN and graph representation learning, have shown great potential in HSI classification problem. However, the existing GCN-based methods have many problems, such as over dependence on the adjacency matrix, usage of a single modal feature, and lower accuracy than the mature CNN method. In this paper, we propose a feature fusion hypergraph convolutional neural network (F2HNN) for HSI classification. F2HNN first generates hyperedges from features of different modalities to construct a hypergraph representing multi-modal features in HSI. Then, the HSI and the extracted hypergraph are input into the hypergraph convolutional neural network for learning. In addition, we proposes three feature fusion strategies. The first strategy is the most basic spatial and spectral feature fusion. The second strategy fuses the spectral features extracted by a pre-trained multilayer perceptron (MLP) with the spatial features to reduce the redundant information of the original spectral features. The third strategy uses the fusion of CNN features, spectral features and spatial features to explore the capabilities of F2HNN. Sufficient experiments on four datasets have proved the effectiveness of F2HNN.
            </p>
            <pre xml:space="preserve" id="MaTGRS2021Bib" class="bibtex" style="display: none;">
@ARTICLE{9590574,
  author={Ma, Zhongtian and Jiang, Zhiguo and Zhang, Haopeng},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  title={Hyperspectral Image Classification using Feature Fusion Hypergraph Convolution Neural Network},
  year={2021},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TGRS.2021.3123423}
}
            </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
              hideblock('MaTGRS2021Abs');
              hideblock('MaTGRS2021Bib');
            </script>
            </td>
        </tr> <!-- Paper End Here -->

        <tr> <!-- An Paper -->
          <p>
              <b>Self-Attention Fusion Module for Single Remote Sensing Image Super-Resolution</b> <a href="https://doi.org/10.1109/IGARSS47720.2021.9553766" target="_blank"><i class="fa fa-external-link"></i></a>
              <br>
              <font size="3pt" face="Georgia"><i>
                  Han Mei, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>, Zhiguo Jiang
              </i></font>
              <br>
              IEEE International Geoscience and Remote Sensing Symposium IGARSS, 2021
              <br>
              <i class="fa fa-file-pdf-o"></i> <a href="source/pdf/article_mei_igarss_2021.pdf" target="_blank">PDF</a>
              <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('MeiIGARSS2021Abs')">Abstract</a> &nbsp;
              <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('MeiIGARSS2021Bib')">BibTeX</a> &nbsp;
          </p>
          <p id="MeiIGARSS2021Abs" class="abstract" style="display: none;">
              Single image super-resolution (SISR) is an important procedure to improve many remote sensing applications. Global features play an important role in pixel generation of SISR. In this paper, we proposed a self-attention fusion module named as SAF module which combines spatial attention and channel attention in parallel to handle this problem. Our self-attention fusion module can be flexibly added to many popular deep-learning-based SISR models to further improve their representation ability and learn global features. Experiments on UC Merced dataset indicate that SAF module can improve the performance of classic SISR models and achieve state-of-the-art super-resolution results.
          </p>
          <pre xml:space="preserve" id="MeiIGARSS2021Bib" class="bibtex" style="display: none;">
@INPROCEEDINGS{9553766,
  author={Mei, Han and Zhang, Haopeng and Jiang, Zhiguo},
  booktitle={2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS},
  title={Self-Attention Fusion Module for Single Remote Sensing Image Super-Resolution},
  year={2021},
  volume={},
  number={},
  pages={2883-2886},
  doi={10.1109/IGARSS47720.2021.9553766}
}
            </pre>
                <script language="javascript" type="text/javascript" xml:space="preserve">
              hideblock('MeiIGARSS2021Abs');
              hideblock('MeiIGARSS2021Bib');
            </script>
            </td>
        </tr> <!-- Paper End Here -->
                </tbody></table>
              </div>
              <div class="get-more" align="right"><a href="publications.html"> More </a></div>
            </div>
          </introduce>
        </div>
        <!-- Slid Page -->
        <div class="col-md-4 sidebar-gutter">
          <aside>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <div class="widget-container widget-main">
              <img src="images/photo/JiangZG.jpg" alt="JiangZG's photo">
              <h4>Zhiguo Jiang</h4>
              <div class="author-title">Professor</div>
              <p>
                <b>Address:</b> 9 South-3rd Street, Shahe University Park, Changping District, Beijing, 102206, China<br>
                <b>E-mail:</b> <a href="mailto:jiangzg@buaa.edu.cn">jiangzg@buaa.edu.cn</a><br>
<!--
                <b>Tel:</b> TBA<br>
                <b>Fax:</b> TBA<br>
-->
                <b>Office:</b> D721, Main Building<br>

              </p>
            </div>
          </div>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <h3 class="sidebar-title">Researchers</h3>
            <div class="widget-container">
              <article class="widget-block">
                <div class="block-image"> <img src="images/photo/ZhangHP.jpg" alt="ZhangHP's photo"> </div>
                <div class="block-body">
                  <h2><a href="https://haopzhang.github.io/" target="_blank">Haopeng Zhang <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>Associate Professor</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:zhanghaopeng@buaa.edu.cn">zhanghaopeng@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
              <article class="widget-block">
                <div class="block-image"> <img src="images/photo/XieFY.jpg" alt="ZhangYS's photo"> </div>
                <div class="block-body">
                  <h2><a href="http://www.sa.buaa.edu.cn/info/1014/4773.htm" target="_blank">Fengying Xie <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>Professor</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:xfy_73@buaa.edu.cn">xfy_73@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
              <article class="widget-block">
                <div class="block-image"> <img src="images/photo/ZhaoDP.jpg" alt="ZhangYS's photo"> </div>
                <div class="block-body">
                  <h2><a href="http://www.sa.buaa.edu.cn/info/1014/4780.htm" target="_blank">Danpei Zhao <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>Associate Professor</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:zhaodanpei@buaa.edu.cn">zhaodanpei@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
              <article class="widget-block">
                  <div class="block-image"> <img src="images/photo/ZhengYS.jpg" alt="ZhengYS's photo"> </div>
                  <div class="block-body">
                    <h2><a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng <i class="fa fa-external-link"></i></a></h2>
                    <div class="icon-meta">
                      <span><i class="fa fa-graduation-cap"></i>Associate Professor</span> <span><i class="fa fa-clock-o"></i> </span>
                      <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:yszheng@buaa.edu.cn">yszheng@buaa.edu.cn</a></span>
                    </div>
                  </div>
                </article>
            </div>
          </div>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <h3 class="sidebar-title">Contact Us</h3>
            <div class="widget-container">
              <p>
                <b>Address:</b> D208, Main Building, 9 South-3rd Street, Shahe University Park, Changping District, Beijing, 102206, China<br>
<!--
                <b>Tel:</b> TBA<br>
                <b>Fax:</b> TBA<br>
-->
              </p>
            </div>
          </div>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <h3 class="sidebar-title">Related Links</h3>
            <div class="widget-container">
              <ul style="list-style: none; padding-left: 10px;">
                <li><i class="fa fa-external-link"></i> <a href="http://www.sa.buaa.edu.cn/info/1014/4780.htm" target="_blank">Zhao's home page</a></li>
                <li><i class="fa fa-external-link"></i> <a href="http://www.sa.buaa.edu.cn/info/1014/4773.htm" target="_blank">Xie's home page</a></li>
                <li><i class="fa fa-external-link"></i> <a href="https://haopzhang.github.io/" target="_blank">Zhang's home page</a></li>
                <li><i class="fa fa-external-link"></i> <a href="https://zhengyushan.github.io" target="_blank">Zheng's home page</a></li>
              </ul>
            </div>
          </div>
          </aside>
        </div>
      </div>
    </section>
    </div><!-- /.container -->

    <footer class="footer">
      <div class="footer-bottom">  
        <i class="fa fa-copyright"></i> ````````````````1
          Copyright 2018. All rights reserved.<br>
        <!-- <i class="fa fa-anchor"></i> <a href="index_x.html"><b>X！</b><i class="fa fa-sign-in"></i></a> -->
      </div>
      
    </footer>


    <!-- Bootstrap core JavaScript
      ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="scripts/jquery.min.js"></script>
    <script src="scripts/bootstrap.min.js"></script>
    <script src="scripts/jquery.bxslider.js"></script>
    <script src="scripts/mooz.scripts.min.js"></script>
    <script src="scripts/togglehide.js"></script>
  </body>
</html>
